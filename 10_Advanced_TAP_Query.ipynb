{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250, style=\"padding: 10px\"> \n",
    "<b>Advanced TAP Queries for DP0 catalogs </b> <br>\n",
    "Last verified to run on 2021-06-25 with LSST Science Pipelines release w_2021_25 <br>\n",
    "Contact authors: Leanne Guy <br>\n",
    "Target audience: All DP0 delegates. <br>\n",
    "Container Size: medium <br>\n",
    "Questions welcome at <a href=\"https://community.lsst.org/c/support/dp0\">community.lsst.org/c/support/dp0</a> <br>\n",
    "Find DP0 documentation and resources at <a href=\"https://dp0-1.lsst.io\">dp0-1.lsst.io</a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit:** Originally developed by Leanne Guy in the context of the Rubin DP0.1. Please consider acknowledging Leanne Guy if this notebook is used for the preparation of journal articles or software releases.\n",
    "\n",
    "TODO: check https://confluence.lsstcorp.org/display/DM/058+Find+all+objects+that+are+varying+with+the+same+pattern+as+a+given+object%2C+possibly+at+different+time\n",
    "and \n",
    "https://github.com/lsst/qserv_testdata/blob/8c635e29b8b42087e18601537d0a588fcfb40485/datasets/case01/queries/0006_transientVarObjNearGalaxy.sql\n",
    "\n",
    "for some good examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "The Rubin Science Platform provides QUERY access to the DP0.1 catalogs via TAP from jupyter notebooks. TAP is a Virtual Observatory protocol for access to catalog data. In this tutorial, we will learn how to exploit some of the more advanced capabilities of ADQL and Qserv to query the DP0.1 archive via TAP. Full TAP documentation can be found [here](https://www.ivoa.net/documents/TAP/). \n",
    "\n",
    "Prerequisities: Have completed and understood notebook 02_Intermediate_TAP_Query\n",
    "\n",
    "This notebook demonstrates how to: <br>\n",
    "1. Introduce advanced ADQL queries and the science they can enable </br>\n",
    "2. Read SQL queries from an external file and execute in Python\n",
    "3. Query data hosted at other archives via TAP, join with DP0.1 data<br>\n",
    "\n",
    "Resources: \n",
    "The following resources may be helpful:\n",
    "1. [Qserv user Guide](https://github.com/lsst/qserv/blob/master/UserManual.md#sub-queries-are-not-supported)\n",
    "2. [IVOA ADQL User Guide](https://www.ivoa.net/documents/ADQL/20180112/PR-ADQL-2.1-20180112.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvo\n",
    "import re\n",
    "\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Astropy\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "# HealPy \n",
    "import healpy as hp\n",
    "\n",
    "# SQL parse tools \n",
    "import sqlparse\n",
    "\n",
    "# Holoviews\n",
    "import holoviews as hv\n",
    "from holoviews import streams, opts\n",
    "from holoviews.operation.datashader import datashade, dynspread, rasterize\n",
    "from holoviews.plotting.util import process_cmap\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Bokeh for plotting\n",
    "from bokeh.io import output_file, output_notebook,  show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "# Set the maximum number of rows to display from pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "                  \n",
    "# Configure bokeh to generate output in notebook cells when show() is called.\n",
    "output_notebook()\n",
    "\n",
    "# Set bokeh as the holoviews backend\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reduce warnings from logger\n",
    "import logging\n",
    "logging.getLogger(\"flake8\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading SQL queries from an external file\n",
    "\n",
    "A common use case is to reuse data returned frm a single query as part of more than on analysis. Hard coding queries in a single notebook makes query reuse difficult. A better strategy is to store ADQL queries in an external file and read them in to any notbeook that will use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Rubin TAP service utilities\n",
    "from lsst.rsp import get_tap_service\n",
    "\n",
    "# Get an instance of the Rubin TAP service\n",
    "rb_service = get_tap_service()\n",
    "assert rb_service is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to DP0.1 queries\n",
    "dp01QueryPath = './queries/dp0-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL file in \n",
    "queryPath = os.path.join(dp01QueryPath, 'dp01Tables.sql')\n",
    "fd = open(queryPath, 'r')\n",
    "sql = fd.read()\n",
    "fd.close()\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire file is read in, including the comments. This string is not executable as is. We will use the python [sqlparse](https://sqlparse.readthedocs.io/) library to parse our SQL statements. The `format` method allows us to strip the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sqlparse.format(sql, strip_comments=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rb_service.search(query).to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load sql queries from a file\n",
    "def loadQuery(path):\n",
    "    \"\"\" Load a query from a file and strip out the comments \"\"\"\n",
    "    fd = open(path, 'r')\n",
    "    sql = fd.read()\n",
    "    fd.close()\n",
    "    return sqlparse.format(sql, strip_comments=True, reindent=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deep dive into advanced Qserv capabilities\n",
    "\n",
    "LSST Query Services (Qserv) provides access to the LSST Database Catalogs in DP0.1 and will the database for all LSST Data Previews and Releases. Qserv has been designed to handle petascale LSST catalogs. \n",
    "\n",
    "Qserv supports standard SQL query language with a few restrictions as follows: \n",
    "1. Sub queries are not supported.\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Advanced ADQL queries with Qserv\n",
    "In notebook `02_Intermediate_TAP_Query` we introduced basic ADQL including queries of single tables, table joins and selection cuts. In this notebook we are going to take that one step further and look at how we can analyse the millions entries in the LSST catalogs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Histograms \n",
    "\n",
    "For petascale datasets, such as that of the LSST,  retrieving millions or billions of entries from Qserv over TAP and then binning or aggregating in a notebook is not efficient and will not scale. Instead, we can use Qserv to reduce and aggregate data via ADQL queries. This can be extremely useful when we want to compute summary statistics across large datasets. \n",
    "\n",
    "In `02_Intermediate_TAP_Query` we saw a simple example of how to bin catagorical data using the 'GROUP BY' ADQL command to group the `Objects` in the `truth_match` catalog by type (1: galaxies, 2:stars, 3: SNe), and the 'COUNT' command to count the number of Objects in each category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is that query again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_truth_type = loadQuery(os.path.join(dp01QueryPath, 'truthTypeCount.sql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We remove the index so that it doesn't show up in the hover tool on the plots.\n",
    "object_types = rb_service.search(query_truth_type).to_table().to_pandas(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time taken by this query is of the order of 10 secs. It would take considerably longer to retrieve the entire truth match catalog and bin it in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the numerical values for each truth type to a more descriptive name\n",
    "# Catalog schema for the truth table can be found at:\n",
    "#  https://dp0-1.lsst.io/data-products-dp0-1/index.html#catalogs\n",
    "object_map = {1: 'Galaxy', 2: 'Star', 3: 'SNe'}\n",
    "object_types['truth_type'] = object_types['truth_type'].map(object_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at computations on non-categorical data. We will run these as asynchronous queries as they will take a few minutes. We will query the forced photometry catalog, joining on the object catalog, to look at the distribution of elliptical Gaussian adaptive moments (pixels^2), given by the parameter `i_base_SdssShape_xy`, for objects detected in a single tract, tract 2723. The parameter `i_base_SdssShape_xy` is specified as a `double` in Qserv, so we will use the SQL `ROUND` function to round the values to 0 decimal places. Thanks to Douglas Tucker for inspiration on these queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT ROUND(fp.i_base_SdssShape_xy, 0) as bin, \"\\\n",
    "        \"COUNT(*) as count \"\\\n",
    "        \"FROM dp01_dc2_catalogs.forced_photometry as fp \"\\\n",
    "        \"JOIN dp01_dc2_catalogs.object as obj ON fp.objectId = obj.objectId \"\\\n",
    "        \"WHERE obj.tract=2723 \"\\\n",
    "        \"GROUP BY bin ORDER BY bin\"\n",
    "#query = loadQuery(os.path.join(dp01QueryPath, 'iBandShapeByTract.sql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breakdown this query. We join the `forced_photometry` catalog with the `object` catalog to select only those objects in tract 2723.  ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = rb_service.submit_job(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "print('Job phase is', job.phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tract_2723 = job.fetch_result().to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tract_2723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binning here is clearly not optimal. Lets look at defining a more appropriate bin size so we can better understand the shape of the distribution. Let's use 10 bins by specifying the width of the bucket. We round each value of `i_base_SdssShape_xy` down to the nearest multiple of 5 (rather than 1 previously) and then group by that rounded value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that the original query contained `NaN` values. Let's exclude them and also apply a quality cut on the i-band results using th `i_good` flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width = 10\n",
    "query = \"SELECT floor(fp.i_base_SdssShape_xy/\" + str(bin_width) + \")*\" + str((bin_width)) + \" as edge, \"\\\n",
    "        \"COUNT(*) as count \" \\\n",
    "        \"FROM dp01_dc2_catalogs.forced_photometry as fp \" \\\n",
    "        \"JOIN dp01_dc2_catalogs.object as obj ON fp.objectId = obj.objectId \" \\\n",
    "        \"WHERE obj.tract=2723 \" \\\n",
    "        \"AND i_good = 1 AND i_base_SdssShape_xy != 'NaN' \" \\\n",
    "        \"GROUP BY edge \" \\\n",
    "        \"ORDER BY edge ASC\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = rb_service.submit_job(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "print('Job phase is', job.phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_tract_2723 = job.fetch_result().to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_tract_2723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changes now ..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT floor(fp.i_base_SdssShape_xy/10.0)*10 as edge, \"\\\n",
    "        \"COUNT(*) as count \" \\\n",
    "        \"FROM dp01_dc2_catalogs.forced_photometry as fp \" \\\n",
    "        \"JOIN dp01_dc2_catalogs.object as obj ON fp.objectId = obj.objectId \" \\\n",
    "        \"WHERE obj.tract=2723 \" \\\n",
    "        \"AND i_good = 1 AND i_base_SdssShape_xy != 'NaN' \" \\\n",
    "        \"GROUP BY edge \" \\\n",
    "        \"ORDER BY edge\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT ROUND(fp.i_base_SdssShape_xy, -2) AS bucket, \"\\\n",
    "        \"COUNT(*) AS COUNT, \"\\\n",
    "        \"RPAD('', LN(COUNT(*)), '*') AS bar \"\\\n",
    "        \"FROM   dp01_dc2_catalogs.forced_photometry as fp \"\\\n",
    "        \"WHERE obj.tract=2723 \"\\\n",
    "        \"AND i_good = 1 AND i_base_SdssShape_xy != 'NaN' \" \\\n",
    "        \"GROUP BY bucket ORDER  BY bucket\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT obj.tract, \n",
    "        \"ROUND(fp.i_base_SdssShape_xy, 1) as bin, \"\\\n",
    "        \"COUNT(*) \" \\\n",
    "        \"FROM dp01_dc2_catalogs.forced_photometry as fp \" \\\n",
    "        \"JOIN dp01_dc2_catalogs.object as obj \" \\\n",
    "        \"ON fp.objectId = obj.objectId \" \\\n",
    "        \"GROUP BY obj.tract \" \\\n",
    "        \"ORDER BY obj.tract ASC\"\n",
    "print(query)\n",
    "results_all_tracts = rb_service.search(query).to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Histogram(binned_tract_2723, \n",
    "             kdims=['edge'], vdims=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on the timing for these two queries. Why does the second take the same amount of time as the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a query to do a logarithmic binning of some forced photometry fluxes in a cone of radius 1.0 degree. \n",
    "query = \"SELECT COUNT(*), FLOOR(LOG10(i_modelfit_CModel_initial_instFlux)) as BIN \"\\\n",
    "        \"FROM dp01_dc2_catalogs.forced_photometry \"\\\n",
    "        \"WHERE CONTAINS(POINT('ICRS', coord_ra, coord_dec), CIRCLE('ICRS', 60.0, -35.0, 1.0))=1 \"\\\n",
    "        \"GROUP BY BIN ORDER BY BIN\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Construct a CMD using ADQL \n",
    "Now we will look at some other more complex uses of the 'GROUP BY' functionality. Construct a CMD with bin size = BIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select\n",
    "  bp_rp_index / 10 as bp_rp,\n",
    "  g_mag_abs_index / 10 as g_mag_abs,\n",
    "  count(*) as n\n",
    "from (\n",
    "     ## DC2 add in a cut on redshift  and some cuts on the quality of the data\n",
    "  select top 1000000 source_id,\n",
    "    floor((phot_g_mean_mag+5*log10(parallax)-10) * 10) as g_mag_abs_index,\n",
    "    floor(bp_rp * 10) as bp_rp_index\n",
    "  from gaiadr2.gaia_source\n",
    "  where parallax_over_error >= 5 and\n",
    "    phot_bp_mean_flux_over_error > 0 and\n",
    "    phot_rp_mean_flux_over_error > 0 and\n",
    "    sqrt(\n",
    "      power(2.5/log(10) / phot_bp_mean_flux_over_error, 2)\n",
    "      + power(2.5/log(10) / phot_rp_mean_flux_over_error, 2)\n",
    "    ) <= 0.05\n",
    "  order by random_index\n",
    ")as subquery\n",
    "group by bp_rp_index, g_mag_abs_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Aggregation methods in ADQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Query archives at external data centres via TAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create TAP Service clients to access data at Rubin and other external data centres.  \n",
    "\n",
    "In notebook '02_Intermediate_TAP_Query' we saw how to use the Rubin provided TAP service to access DP0.1 data. Similarly, \n",
    "most astronomical archives proved a TAP service to access the data stored at their archive.  We are going to learn at how to access the catalogs \n",
    "stored at other astronimical archives over TAP from the Rubin Science Platform. \n",
    "\n",
    "**Hazard Warning:** Not all ADQL functionality is supported yet in the DP0 RSP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create a TAP service to query external archives\n",
    "For this example, we'll use 1) The Gaia Archive's TAP service at ESAC, 2) NOIRLab's AstroDataLab and 3) <X>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaia Archive TAP service \n",
    "gea_tap_url = \"https://gea.esac.esa.int/tap-server/tap\"\n",
    "gea_service = pyvo.dal.TAPService(gea_tap_url)\n",
    "assert gea_service is not None\n",
    "assert gea_service.baseurl == gea_tap_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Gaia archive to see what data they expose\n",
    "query = \"select * from tap_schema.schemas\"\n",
    "gea_schemas = gea_service.search(query).to_table()\n",
    "gea_schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaia archive make available Gaia DR1, DR2 and Gaia Early DR3 over TAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datalab TAP service\n",
    "dl_tap_url = \"https://datalab.noirlab.edu/tap\"\n",
    "dl_service = pyvo.dal.TAPService(dl_tap_url)\n",
    "assert dl_service is not None\n",
    "assert dl_service.baseurl == dl_tap_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the DataLab TAP schema to see what data they expose\n",
    "query = \"select * from tap_schema.schemas\"\n",
    "dl_schema = dl_service.search(query).to_table()\n",
    "dl_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLab TAP service makes available a large number of catalogs from many surveys that are hosted at the NOIRLab Community Science Data Centre (CSDC), including Gaia DR2 and EDR3 that we saw above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Query and retrieve data from Gaia DR2 for variable stars and plot  .. or maybe  show that the galazy is rotating (RV/DR2)\n",
    "\n",
    "1) Query for a given 2 source Ids and get the URL to the epoch data. Then retrieve the VOTable from the link and plot the time series. \n",
    "\n",
    "2) Show the density distribution of radial velocities in Gaia DR2.  (Gaia Collaboration, Katz et al. 2019 A&A 622, A205; adapted Fig. 7). We will reproduce some of the content in Gaia Data Release 2 Mapping the Milky Way disc kinematics. See also this [SciAm article]( https://www.americanscientist.org/article/gaia-reveals-the-milky-way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First explore the Gaia schema to find details of the tables\n",
    "query = \"SELECT * FROM tap_schema.tables \"\\\n",
    "        \"WHERE tap_schema.tables.schema_name = 'gaia_dr2' AND table_name like '%source%' \" \n",
    "gaia_edr3_tables= dl_service.search(query).to_table().to_pandas()\n",
    "#print(gaia_edr3_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many RV are there in the Gaia DR2 catalog. \n",
    "query = \"select count(*) as num_rv_stars from gaiadr2.gaia_source where radial_velocity IS NOT NULL AND ABS(radial_velocity) < 550\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rv_stars = gea_service.search(query)\n",
    "# assert 7224631  == num_rv_stars, f\"Expected 7224632, got {num_rv_stars}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the source table to retrieve the average distribution of Gaia DR2 radial velocities in Galactic Coordinates. \n",
    "# Note that if we queried the source table to retrieve all 7.2 million the RVs and then aggregatted them in this NB, you would kill it. \n",
    "# Instead, we will transform the Gaia source_id for those sources with a radial velocityto HEALPix number using the built \n",
    "# in Gaia archive function GAIA_HEALPIX_INDEX within the ADQL query \n",
    "# and then group the Gaia sources by healpix and compute the average radial This demonstrates the power of ADQL's aggregation functionality \n",
    "# It also shows that remote built in functions are also available over TAP. \n",
    "# These numbers have been aggregated in sky bins of 0.84 deg2 (level 6 HEALPix) and their mean value is shown.\n",
    "# HP7: 0.21 sq deg. \n",
    "# Always read the Gaia DR2 Primer before workng with Gaia data.  https://www.cosmos.esa.int/web/gaia/gaia-dr2-primer\n",
    "query = \"SELECT GAIA_HEALPIX_INDEX(7, source_id) AS healpix7, avg(radial_velocity) AS avg_radial_velocity \" \\\n",
    "        \"FROM gaiadr2.gaia_source \"\\\n",
    "        \"WHERE radial_velocity IS NOT NULL \"\\\n",
    "        \"AND ABS(radial_velocity) < 550 \" \\\n",
    "        \"GROUP BY healpix7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could take a while so let's run it as an asynchronous query. Note that queries that take too long will timeout. \n",
    "job = gea_service.submit_job(query)\n",
    "print(f\"'Job with URL {job.url} is in {job.phase} phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause notebook execution until the asynchronous job finshes \n",
    "job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "print('Job phase is', job.phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch results \n",
    "gaia_rv_density_distribution = job.fetch_result().to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a much more manageable dataset and a lot of the work of aggregation has been done within the Gaia archive centre. \n",
    "print(len(gaia_rv_density_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Plot \n",
    "Plot the RVs. This figure was published in Gaia DR2, Gaia Collaboration, Katz et al. 2019 A&A 622, A205; adapted Fig. 7). It is not exactly the same\n",
    "\n",
    "A reminder of some heapix basics. We will use the healpy package for healix calculations. The Gaia archive provides a \n",
    "npix = 12 * nside ** 2\n",
    "nside = 2**order (level). So for heapix level 7. nside is 128. \n",
    "\n",
    "hp.nside2npix\n",
    "hp.npix2order\n",
    "hp.order2npix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside=aphp.level_to_nside(7)\n",
    "nside = hp.order2nside(7)\n",
    "m = np.zeros(hp.nside2npix(nside))\n",
    "idx = np.array(gaia_rv_density_distribution['healpix7'])\n",
    "counts = np.array(gaia_rv_density_distribution['avg_radial_velocity'])\n",
    "m[idx] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size': 10})\n",
    "\n",
    "hp.mollview(m, nest='True', coord=['C','G'], \n",
    "            title=\"Gaia DR2 Average RV\", \n",
    "            cmap=mpl.cm.jet,\n",
    "            norm=\"hist\")\n",
    "hp.graticule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gaia Sky Density distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Sky density distribution\n",
    "query = \"SELECT gaia_healpix_index(7, source_id) AS healpix7, count(*)/0.9161 AS stars_per_sq_deg \" \\\n",
    "        \"FROM gaiaedr3.gaia_source \" \\\n",
    "        \"GROUP BY healpix7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could take a while so let's run it as an asynchronous query. Note that queries that take too long will timeout. \n",
    "job = gea_service.submit_job(query)\n",
    "print(f\"'Job with URL {job.url} is in {job.phase} phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.wait(phases = ['COMPLETED', 'ERROR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = job.fetch_result().to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_sky = np.zeros(hp.nside2npix(hp.order2nside(7)))\n",
    "gaia_sky[np.array(results['healpix7'])] = np.array(results['stars_per_sq_deg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(gaia_sky,\n",
    "            nest=True, coord=['C','G'],\n",
    "            norm='log', title='Gaia Sky Density')\n",
    "# hp.graticule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Synergies between LSST and Gaia\n",
    "We can clearly see the complementarity between LSST and Gaia by looking a the distribution of uncertainties for photometry, astrometry and proper motion as a function of magnitude. Here we use the DC2 data and data from Gaia DR2/EDR3 to plot the uncertainty as a function of magnitude for for both (reference original plots)\n",
    "\n",
    "https://github.com/agabrown/PyGaia/blob/master/pygaia/errors/astrometric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT objectid, ra, dec, mag_r, magerr_r, mag_r_cModel, magerr_r_cModel \"\\\n",
    "        \"FROM dp01_dc2_catalogs.object \" \\\n",
    "        \"WHERE mag_r_cModel != 'NaN' \" \\\n",
    "        \"AND mag_r_cModel < 29\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rb_service.search(query, maxrec=20)\n",
    "results = results.to_table().to_pandas()\n",
    "#lsst_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT sourceid, ra, dec, parallax_error, phot_g_mean_mag, parallax_over_error, \"\\\n",
    "        \"pm, pmra, pmra_error, pmdec, pmdec_error \" \\\n",
    "        \"FROM gaiaedr3.gaia_source \" \\\n",
    "        \"WHERE mag_r_cModel != 'NaN' \" \\\n",
    "        \"AND mag_r_cModel < 29\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT TOP 10 ASTROMETRIC_PARAMETER_ERROR(\" \\\n",
    "        \"ra_error, dec_error, parallax_error, pmra_error, pmdec_error, \"\\\n",
    "        \"ra_dec_corr, ra_parallax_corr, ra_pmra_corr, ra_pmdec_corr, dec_parallax_corr, \"\\\n",
    "        \"dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, \"\\\n",
    "        \"parallax, radial_velocity, radial_velocity_error) \" \\\n",
    "        \"FROM gaiadr2.gaia_source\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gaia = gea_service.search(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
